{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dask**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we dive into dask, lets look at a simple usecase:\n",
    "### **Let's say you want to figure out the best channels on Youtube to advertise a product on, across the globe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"data\")\n",
    "COLS = [\n",
    "    'video_id',\n",
    "    'trending_date',\n",
    "    'title',\n",
    "    'channel_title',\n",
    "    'category_id',\n",
    "    'publish_time',\n",
    "    'tags',\n",
    "    'views',\n",
    "    'likes',\n",
    "    'dislikes',\n",
    "    'comment_count',\n",
    "    'thumbnail_link',\n",
    "    'comments_disabled',\n",
    "    'ratings_disabled',\n",
    "    'video_error_or_removed'\n",
    "]\n",
    "\n",
    "df1 = pd.read_csv(os.path.join(DATA_DIR, \"youtube\", \"USvideos.csv\"), usecols=COLS)\n",
    "df2 = pd.read_csv(os.path.join(DATA_DIR, \"youtube\", \"GBvideos.csv\"), usecols=COLS)\n",
    "\n",
    "top_5_US_channels = df1.groupby(\"channel_title\").sum().reset_index()[[\"channel_title\", \"views\"]].sort_values(by=\"views\", ascending=False).head(5)\n",
    "top_5_UK_channels = df2.groupby(\"channel_title\").sum().reset_index()[[\"channel_title\", \"views\"]].sort_values(by=\"views\", ascending=False).head(5)\n",
    "\n",
    "df3 = pd.concat([df1, df2])\n",
    "top_5_across_US_and_UK_channels = df3.groupby(\"channel_title\").sum().reset_index()[[\"channel_title\", \"views\"]].sort_values(by=\"views\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_across_US_and_UK_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we have n geographic regions, we have to create n dataframes - not really pretty\n",
    "\n",
    "### We are also restricted by the size of the dataset that can fit in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **So how does dask help exactly?**\n",
    "\n",
    "### Dask = Dask Collections + Dask Task Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask offers parallel versions for familiar data structures in pandas & numpy like\n",
    "\n",
    "1. Dask Array (parallel numpy arrays)\n",
    "2. Dask Bag (parallel python lists)\n",
    "3. Dask Dataframe (parallel pandas dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask also has few helpful parallelization primitives\n",
    "\n",
    "- Dask Delayed (lazy parallelism)\n",
    "- Dask Futures (real-time parallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/dask-overview.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how we can solve our previous problem with dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "from distributed import Client\n",
    "client = Client()\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"youtube\", \"*.csv\"),\n",
    "    encoding=\"latin1\"\n",
    ")\n",
    "\n",
    "df.compute().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"channel_title\").sum().reset_index()[[\"channel_title\", \"views\"]].nlargest(n=5, columns=\"views\").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at how dask arrays are different from numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "x = da.arange(100, chunks=20) # each chunk has 20\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.npartitions, x.chunks, x.chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in range(5):\n",
    "    print(np.array(x.blocks[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.map_blocks(lambda p: p + 2).compute() # map_blocks only accepts funcs which work element wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have already seen dask dataframes but what exactly are dask bags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "y = db.from_sequence(range(10), npartitions=4)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.map(lambda x: x + 4).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets parallelize a simple operation, say parsing some CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "regions = [\"CA\", \"DE\", \"FR\", \"GB\", \"IN\", \"JP\", \"KR\", \"MX\", \"RU\", \"US\"]\n",
    "\n",
    "def find_count_in_csv(region):\n",
    "    csv_path = os.path.join(\"data\", \"youtube\", f\"{region}videos.csv\")\n",
    "    with open(csv_path, 'rt', encoding=\"latin1\") as csvfile:\n",
    "        rows = csv.reader(csvfile)\n",
    "        return region, sum(1 for _ in rows)\n",
    "\n",
    "tuples = db.from_sequence(regions).map(find_count_in_csv).compute()\n",
    "dict(tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can use delayed and futures to parallelize existing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed, compute\n",
    "import time\n",
    "from random import randrange\n",
    "\n",
    "# @delayed\n",
    "def func1(n):\n",
    "    time.sleep(1)\n",
    "    return n\n",
    "\n",
    "# @delayed\n",
    "def func2(n):\n",
    "    time.sleep(2)\n",
    "    return n ** 2\n",
    "\n",
    "# @delayed\n",
    "def func3(a, b):\n",
    "    return a + b\n",
    "\n",
    "def main_func(x, y):\n",
    "    p = func1(x)\n",
    "    q = func2(y)\n",
    "    r = func3(p, q)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time main_func(4,5)\n",
    "# %time main_func(4,5).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = client.submit(func1, 4)\n",
    "c2 = client.submit(func2, 5)\n",
    "c3 = client.submit(func3, c1, c2)\n",
    "%time c3.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask futures are similar to Python 3+'s async/await and in fact extend it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import numpy as np\n",
    "\n",
    "def func(x):\n",
    "    return np.tan(x) * np.arctan(x)\n",
    "\n",
    "%time da.arange(10 ** 7).map_blocks(func, dtype=float).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not all functions in numpy/pandas are supported by dask\n",
    "\n",
    "- [Dask Array Scope](https://docs.dask.org/en/latest/array.html#scope)\n",
    "- [Dask Bag Limitations](https://docs.dask.org/en/latest/bag.html#known-limitations)\n",
    "- [Dask Dataframe Scope](https://docs.dask.org/en/latest/dataframe.html#scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise**\n",
    "\n",
    "Try parallelizing the python code you wrote earlier without using prange from numba. Do you notice any improvements?\n",
    "\n",
    "Also see if you can find another implementation for \"func\".\n",
    "\n",
    "**If you find any improvement, feel free to tweet about your experience with the handle @pyconfhyd**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
