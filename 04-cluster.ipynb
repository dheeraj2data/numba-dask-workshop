{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dask Distributed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So far we have ran all the code on a single machine - albeit with a decent number of cores\n",
    "\n",
    "### How does this translate to a cluster? \n",
    "\n",
    "### The dask-distributed package provisions many ways to help run dask in a cluster. We'll look at how we can create a simple SSH Cluster."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#  Dont execute this - it wont work. You can start by creating hosts in /etc/hosts \n",
    "\n",
    "HOSTS = [\n",
    "    \"scheduler\",\n",
    "    \"worker1\",\n",
    "    \"worker2\",\n",
    "    \"worker3\",\n",
    "    \"worker4\"\n",
    "]\n",
    "\n",
    "scheduler_ip = 0.0.0.0\n",
    "\n",
    "from dask.distributed import Client, SSHCluster\n",
    "\n",
    "cluster = SSHCluster(\n",
    "    hosts=HOSTS, \n",
    "    ssh_username=\"root\",\n",
    "    remote_python=\"/root/anaconda/envs/dask/bin/python\",\n",
    "    scheduler_addr=scheduler_ip,\n",
    "    scheduler_port=80\n",
    ")\n",
    "\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import numpy as np\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "# Alternatively, you can use dask-ssh CLI to connect \n",
    "# client = Client(\"tcp://206.189.136.196:8786\")\n",
    "\n",
    "def func(x):\n",
    "    return np.tan(x) * np.arctan(x)\n",
    "\n",
    "%time da.arange(10 ** 7).map_blocks(func, dtype=float).compute()\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can simple use dask-scheduler and dask-worker CLI commands to create a cluster and manage it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask can be run on kubernetes as well and has decent integration with AWS, Azure & GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see if we can combine numba and dask together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit, int32\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from math import tan, atan\n",
    "from dask import delayed, compute\n",
    "\n",
    "class DistributedComputation(object):\n",
    "    def __init__(self, n_workers=4):\n",
    "        super().__init__()\n",
    "        self.n_workers = n_workers\n",
    "        self.cluster = LocalCluster(\n",
    "            n_workers=n_workers, \n",
    "            processes=True, \n",
    "            threads_per_worker=1\n",
    "        )\n",
    "\n",
    "    def execute(self, func, *args, **kwargs):\n",
    "        with Client(self.cluster) as client:\n",
    "            return func(*args, **kwargs)\n",
    "\n",
    "@delayed\n",
    "@jit(int32(int32), nopython=True)\n",
    "def fast_func(N):\n",
    "    result = 0\n",
    "    for i in range(N ** 7):\n",
    "        result += tan(i) * atan(i)\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    d = DistributedComputation(n_workers=16)\n",
    "    fast_func(0) # first call to initiate JIT\n",
    "    list_of_delayed_objs = (d.execute(fast_func, i) for i in range(1, 11))\n",
    "    print(compute(*list_of_delayed_objs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowing which worker is doing what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "client = Client(\"tcp://192.168.0.105:8786\") # spin up two workers and a scheduler in your terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import get_worker, wait\n",
    "import json\n",
    "\n",
    "x = da.arange(100, chunks=50)\n",
    "\n",
    "def do_something(x):\n",
    "    worker = get_worker()\n",
    "    print(f\"{worker.id} is the worker id\")\n",
    "    print(list(worker.data.values()))\n",
    "    return x\n",
    "\n",
    "x.map_blocks(do_something, dtype=int).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise**\n",
    "\n",
    "Try creating a scheduler locally and use the IP address to create dask workers. Try scaling up manually and see how it affects performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
